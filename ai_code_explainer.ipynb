{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72fcaf0-2718-4a03-a632-a4ed1b43304e",
   "metadata": {},
   "source": [
    "# AI Code Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b83fd-3c8c-44fc-9f9b-58ad8efccee4",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94661efc-2912-4e3b-ac85-4bf79255bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "from typing import Literal\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10b92ee2-7af4-4589-9413-c29f49dc81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_response(stream, type:Literal[\"ollama\", \"openai\"]=\"ollama\"):\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        if type == \"ollama\":\n",
    "            response += chunk.choices[0].delta.content\n",
    "        else:\n",
    "            response += chunk.choices[0].delta.content or \"\"\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a006639-f953-42d0-b9c6-bcf589cc67a5",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0d5df27b-cdb1-4e8d-8a36-7ab6a33cef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_code = \"\"\"yield from {book.get(\"author\") for book in books if book.get(\"author\")}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cce2d360-ea97-46f2-96de-3bd2a8354c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an expert programmer, especially in Python. \\\n",
    "You can explain the code given to you easily so that even non-tech savvy people can understand your explanation. \\\n",
    "Give your explanation in Markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ebce3725-2525-4dc1-970e-d67aa852168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the Python code.\n",
      "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "Explain the code.\n"
     ]
    }
   ],
   "source": [
    "def create_user_prompt(python_code:str) -> str:\n",
    "    user_prompt = \"Here is the Python code.\\n\"\n",
    "    user_prompt += f\"{python_code}\\n\"\n",
    "    user_prompt += \"Explain the code.\" # and provide recommendations for improvement if possible\n",
    "    return user_prompt\n",
    "    \n",
    "print(create_user_prompt(python_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "128bbd83-46a3-48e7-875d-77dff647ab3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert programmer, especially in Python. You can explain the code given to you easily so that even non-tech savvy people can understand your explanation. Give your explanation in Markdown.\n",
      "\n",
      "Here is the Python code.\n",
      "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "Explain the code.\n"
     ]
    }
   ],
   "source": [
    "def create_ollama_prompt(python_code:str, system_prompt:str=system_prompt) -> str:\n",
    "    ollama_prompt = f\"{system_prompt}\\n\\n\"\n",
    "    ollama_prompt += create_user_prompt(python_code)\n",
    "    return ollama_prompt\n",
    "\n",
    "print(create_ollama_prompt(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d2e64-b0a1-4eb7-bf92-8f3b0cea4157",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "30d419c2-f906-4db9-888e-a8f877b2c465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "# Check ollama connection\n",
    "# !ollama serve\n",
    "\n",
    "ollama_api = \"http://localhost:11434/\"\n",
    "req = requests.get(ollama_api)\n",
    "print(req.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9bbc6aad-8b20-4883-8e5b-6d69e885ab0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED       \n",
      "llama3.2:latest     a80c4f17acd5    2.0 GB    35 minutes ago    \n",
      "deepseek-r1:1.5b    a42b25d8c10a    1.1 GB    3 weeks ago       \n",
      "tinyllama:latest    2644915ede35    637 MB    3 weeks ago       \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d51e130-97d9-4aa7-8965-8d7aaf706613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ‹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ™ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ´ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â § \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ‡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â � \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ‹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ™ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest â ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Pull ollama model\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7164a6a8-2fc6-48a6-89c5-b9566c971e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "OLLAMA_API_KEY = \"ollama\"\n",
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "\n",
    "ollama_via_openai = OpenAI(base_url=BASE_URL, api_key=OLLAMA_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e375259-964a-4c0d-846c-3b21fb48e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_explain_code(python_code:str):\n",
    "    stream = ollama_via_openai.chat.completions.create(\n",
    "        model=OLLAMA_MODEL,\n",
    "        messages=[{\"role\":\"user\", \"content\":create_ollama_prompt(python_code)}],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    streaming_response(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "39273259-e687-46ce-a003-ec1d3aaac29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Breaking Down the Code**\n",
       "==========================\n",
       "\n",
       "The given Python code uses a concept called **generators**, which allow us to create an iterator that produces a sequence of values without having to store them all in memory at once.\n",
       "\n",
       "**Let's Break it Down:**\n",
       "\n",
       "### 1. `yield from`\n",
       "\n",
       "The keyword `yield from` is used to delegate the generation of an iterable to another iterable. It allows us to \"yield\" multiple iterables at once, rather than just one.\n",
       "\n",
       "Think of it like passing a list of tasks to multiple workers. Each worker processes their task independently and returns their results, which are then combined into a single output.\n",
       "\n",
       "### 2. `{book.get(\"author\") for book in books if book.get(\"author\")}`\n",
       "\n",
       "This is an example of a **list comprehension** or **generator expression**, depending on how you look at it.\n",
       "\n",
       "*   `for book in books`: We're iterating over each **item** (e.g., dictionary) in the `books` list.\n",
       "*   `book.get(\"author\")`: For each item, we're accessing its `\"author\"` key and returning the value associated with that key.\n",
       "*   `if book.get(\"author\")`: We're filtering out items that don't have an `\"author\"` key set.\n",
       "\n",
       "The resulting expression produces a sequence of author names, one for each book in the `books` list.\n",
       "\n",
       "### 3. `yield from ...`\n",
       "\n",
       "We wrap this generator expression with another `yield from` statement to delegate its output to... the original `yield from` statement!\n",
       "\n",
       "Think of it like passing a task list to multiple workers, and then aggregating their outputs into a final result.\n",
       "\n",
       "**Putting It All Together**\n",
       "---------------------------\n",
       "\n",
       "When we combine these two elements, the entire code does the following:\n",
       "\n",
       "*   Iterate over each book in the `books` list.\n",
       "*   For each book, try to get its author name using the `get()` method. If it's not present, skip that iteration altogether.\n",
       "*   Pass the resulting author names to the outer generator expression (the first `yield from`, which itself yields to us).\n",
       "\n",
       "**Result:**\n",
       "The result is a sequence of only those author names that were found within one (or more) books in the list.\n",
       "\n",
       "Here's how you might write this with descriptive variable names for better readability:\n",
       "\n",
       "python\n",
       "def books_names(books):\n",
       "    # Get authors' names from the input books\n",
       "    author_list = [\n",
       "        book.get(\"author\")\n",
       "        for book in books\n",
       "        if \"author\" in book\n",
       "    ]\n",
       "    yield from [author for author, book in zip(author_list, books) if book.get(\"author\")]\n",
       "\n",
       "\n",
       "This example would iterate over a list of books dictionaries and return only the author names present within each book."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama_explain_code(python_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d1b0e-ab59-4699-9b04-fe8416f04a6e",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "242876b6-577b-4300-920d-43b4cf87fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a76fb4a5-dccd-4cec-81b2-7389811f90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_explain_code(python_code:str):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\":system_prompt},\n",
    "            {\"role\":\"user\", \"content\":create_user_prompt(python_code)}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    streaming_response(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "999e07f5-a907-4b72-9920-1336dba3df65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Let's break down the code snippet you provided step by step in a way that’s easy to understand.\n",
       "\n",
       "### Code Explanation\n",
       "\n",
       "python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "\n",
       "1. **`yield from`:**\n",
       "   - This is a Python feature used in generator functions. A generator function is a special type of function that can pause its execution and return a value, and then later continue from where it left off. \n",
       "   - The `yield from` statement is used to yield all values from another iterable (like a list, a set, or another generator). This means it will produce each item one by one without needing a loop.\n",
       "\n",
       "2. **`{ ... for book in books if book.get(\"author\")}`:**\n",
       "   - This part of the code is what we call a *set comprehension*. A set is a collection of unique items in Python.\n",
       "   - The `for book in books` means that we are looking at each item (called `book`) inside a collection called `books`. Think of `books` as a list of dictionaries where each dictionary represents a book with various attributes.\n",
       "   - The `if book.get(\"author\")` part checks if the book has an \"author\" key. The `get(\"author\")` method retrieves the value of the \"author\" key if it exists. If the key does not exist, it returns `None`.\n",
       "   - The entire expression creates a set of unique authors from the `books` collection. Only books that have an author will be included.\n",
       "\n",
       "### Putting It All Together\n",
       "\n",
       "- The complete line of code is creating a generator that produces each unique author from the list of books. \n",
       "- If a book does not have an author, it will be ignored.\n",
       "- This is useful when you want to collect authors without duplicates and generate them when needed.\n",
       "\n",
       "### Example\n",
       "\n",
       "Imagine you have a list of books like this:\n",
       "\n",
       "python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book 3\"},                   # No author\n",
       "    {\"title\": \"Book 4\", \"author\": \"Author A\"}, # Duplicate author\n",
       "]\n",
       "\n",
       "\n",
       "When you run the provided code, the generator will yield **\"Author A\"** and **\"Author B\"**, and it will not yield anything for the book without an author or duplicate values.\n",
       "\n",
       "### Summary\n",
       "\n",
       "In short, the given code is a Python line that efficiently gathers all unique authors from a collection of books and allows us to retrieve them one at a time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openai_explain_code(python_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0631655a-6014-45bf-afca-74b4d37d8565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
